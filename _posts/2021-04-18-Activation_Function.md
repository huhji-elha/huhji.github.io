---
layout: post
title:  "Introduce Activation Functions on Machine Learning"
date:   2021-04-18 17:42:23 +0530
categories: Sigmoid ReLU Leaky-ReLU ELU SELU
use_math: true
---
ğŸ”Œ ë”¥í•œ ëŸ¬ë‹ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•˜ì—¬

_____________________________________




# Activation function

## Why need Activation function?

Loss Functionì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ê¸°ë³¸ì ì¸ ì¸ê³µì‹ ê²½ë§ì€ ì„ í˜• ê²°í•© í˜•íƒœë¡œ ë˜ì–´ìˆë‹¤. (ex. $f(g(h(x)))$). 
node ê°’ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ê³  ë”í•˜ëŠ” í˜•íƒœì´ê¸° ë•Œë¬¸ì— ì´ì „ Layerì™€ ë‹¤ìŒ Layerì˜ ê´€ê³„ ë˜í•œ ì„ í˜• ê²°í•©ì´ë‹¤. \
Layer ì‚¬ì´ì— ë¹„ì„ í˜•ì„±(nonlinear)ì„ ì¶”ê°€í•´ì£¼ì§€ ì•Šìœ¼ë©´ ì•„ë¬´ë¦¬ Layerë¥¼ ê¹Šê²Œ ìŒ“ì•„ë„ í•˜ë‚˜ì˜ Layerì™€ ë™ì¼í•´ì§„ë‹¤. 

ë”°ë¼ì„œ ë³µì¡í•œ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ Activation functionì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•œë‹¤.

ì´ë¡ ì ìœ¼ë¡œ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ê°€ ìˆëŠ” ì¶©ë¶„íˆ ê¹Šì€ ì‹ ê²½ë§ì€ ì–´ë–¤ ì—°ì† í•¨ìˆ˜(continuous function)ë„ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.

Activation Functionì€ ì¢…ë¥˜ê°€ ë§ê³ , í•´ê²°í•˜ê³ ì í•˜ëŠ” Taskì— ë”°ë¼ íŠœë‹ì„ í•´ì£¼ì–´ì•¼ í•  ë•Œë„ ìˆë‹¤.

- Sigmoid â€” $\sigma{x} = {1 \over 1+e^{-x}}$
- tanh â€” $tanh(x)$
- ReLU â€” $max(0, x)$
- Leaky ReLU â€” $max(ax, x)$
- Maxout â€” $max(w_1^T + b_1, w_2^Tx + b_2)$


ì‹ ê²½ë§ì„ í›ˆë ¨í•  ë•Œ, í•œ Epochì´ ëë‚˜ê³  Loss Functionì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ëœë‹¤. 
ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ í•™ìŠµì€ Loss ìì²´ì˜ ìµœì í™”ë¥¼ í‘¸ëŠ” ë¬¸ì œë¡œë„ ë³¼ ìˆ˜ ìˆë‹¤. 

Activation Functionì„ ì‚¬ìš©í•´ NonLinearí•œ ë°ì´í„° ë¶„í¬ê°€ í•™ìŠµë˜ë©´ Loss Functionì˜ convexí•œ íŠ¹ì„±ì´ ê¹¨ì§ˆ ìˆ˜ ìˆë‹¤. 
ë”°ë¼ì„œ ì–´ë–¤ Activation functionì„ ì‚¬ìš©í•˜ëŠëƒê°€ ìˆ˜ë ´ì„±ì´ë‚˜ í•™ìŠµ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤. 

     
     
## 1. Sigmoid

<span style="display:block;text-align:center">$\sigma{x} = {1 \over 1+e^{-x}}$</span>

<span style="display:block;text-align:center">ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</span>

<span style="display:block;text-align:center">$f'(x) = f(x)(1-f(x))$</span>

í•¨ìˆ˜ê°’ì´ 0ê³¼ 1 ì‚¬ì´ì— ìˆë‹¤ëŠ” ê²ƒê³¼ ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ ì¥ì ì´ë‹¤.
outputì´ 0ê³¼ 1ë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— ì£¼ë¡œ Classification ë¬¸ì œì—ì„œ ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ë§ì´ ì‚¬ìš©ëœë‹¤.

<span style="display:block;text-align:center">![sigmoid](https://user-images.githubusercontent.com/59910975/115139255-c57a3f00-a06b-11eb-94cf-a5068086ff40.png)</span>

Sigmoidì˜ ë‹¨ì ì€ back propagationì„ ì ìš©í•˜ë©´ ì·¨ì•½í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. 
ë¯¸ë¶„í•¨ìˆ˜ë¥¼ ë³´ë©´ xê°€ ë§¤ìš° í¬ê±°ë‚˜ ë§¤ìš° ì‘ì„ ë•Œ ëª¨ë‘ 0ì— ê·¼ì‚¬í•˜ê²Œ ë˜ëŠ”ë°, ì´ëŠ” Gradient Vanishing í˜„ìƒ(í•™ìŠµ ì—°ì‚° ì¤‘ì— 0 ê°’ì´ ë§ì´ ê³±í•´ì§€ë¯€ë¡œ ë„¤íŠ¸ì›Œí¬ê°€ ì£½ëŠ” ë¬¸ì œ) ì„ ì¼ìœ¼í‚¨ë‹¤.

ë‹¤ìŒì€ Numpyë¡œ êµ¬í˜„í•œ Sigmoid í•¨ìˆ˜ì´ë‹¤.

```python
import matplotlib.pyplot as plt
import numpy as np

# sigmoid function
t = np.linspace(-10, 10, 100)
sig = 1 / (1 + np.exp(-t))

# draw sigmoid graph
plt.figure(figsize=(8,6))
plt.axhline(y=0, color="black", linestyle="--")
plt.axhline(y=0.5, color="black", linestyle=":")
plt.axhline(y=1.0, color="black", linestyle="--")
plt.axvline(color="grey")
plt.plot(t, sig, linewidth=2, label=r"$\sigma(t) = \frac{1}{1 + e^{-t}}$", color="CornflowerBlue")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("Sigmoid Activation function")
plt.show()
```

## 2. Hyperbolic tangent

<span style="display:block;text-align:center">$tanh(x) = {sinh(x) \over cosh(x)} = {e^{2x}-1 \over e^{2x}+1}$</span>


<span style="display:block;text-align:center">ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</span>

<span style="display:block;text-align:center">$f'(x) = 1-(f(x))^2$</span>

Sigmoid í•¨ìˆ˜ì²˜ëŸ¼ tanh í•¨ìˆ˜ë„ Sì ëª¨ì–‘ì´ê³  ì—°ì†ì ì´ë©° ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤. \
ì‚¬ì‹¤ìƒ Sigmoid í•¨ìˆ˜ë¥¼ í‰í–‰ ì´ë™í•œ ê²ƒê³¼ ê°™ë‹¤.

<span style="display:block;text-align:center">![tangent](https://user-images.githubusercontent.com/59910975/115139256-c57a3f00-a06b-11eb-8634-d0d4a6cd1d71.png)</span>

ì¶œë ¥ ë²”ìœ„ëŠ” -1ê³¼ 1 ì‚¬ì´ì´ë‹¤. ë”°ë¼ì„œ í›ˆë ¨ ì´ˆê¸°ì— ê° ì¸µì˜ ì¶œë ¥ì„ ì›ì  ê·¼ì²˜ë¡œ ëª¨ìœ¼ëŠ” ê²½í–¥ì´ ìˆì–´ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë„ë¡ í•´ì£¼ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. 

RNNì´ë‚˜ LSTM ë“± Recurrent Neural Netì—ì„œ ìŠí˜€ì ¸ì•¼ í•˜ëŠ” ê°’ê³¼ ê¸°ì–µí•´ì•¼ í•˜ëŠ” ê°’ì„ ê²°ì •í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ì˜ë¯¸ë¡œë„ ì‚¬ìš©ëœë‹¤.

```python
#tangent function
t = np.linspace(-10, 10, 100)
tanh = (np.exp(2*t)-1) / (np.exp(2*t)+1)

# draw tangent graph
plt.figure(figsize=(8,6))
plt.axhline(y=-1.0, color="black", linestyle="--")
plt.axhline(y=0, color="grey")
plt.axhline(y=1.0, color="black", linestyle="--")
plt.axvline(color="grey")
plt.plot(t, tanh, linewidth=2, label=r"$\tanh(t) = \frac{e^{2t}-1}{e^{2t}+1}$", color="darkslateblue")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("Hyperbolic Tangent Activation Function")
plt.show()
```

       


## 3. ReLU

<center>$$ReLU(x) = \begin{cases}x & x>0 \\ 0 & x\le 0 \end{cases}$$</center>

<span style="display:block;text-align:center">ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</span>

<center>$$f'(x) = \begin{cases}1 & x>0 \\ 0 & x \le 0 \end{cases}$$</center>

ReLUí•¨ìˆ˜ëŠ” 0ì„ ê¸°ì¤€ìœ¼ë¡œ ì–‘ìª½ì´ ì„ í˜•í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— Sigmoidë³´ë‹¤ ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ì–‘ìª½ ê·¸ë˜í”„ë¥¼ ë¯¸ë¶„í•˜ë©´ 0 ë˜ëŠ” 1ì´ê¸° ë•Œë¬¸ì— 0ë³´ë‹¤ í° ê°’ì„ ë‹¤ìŒ Layerë¡œ ê·¸ëŒ€ë¡œ ë‚´ë³´ë‚´ëŠ” í˜•íƒœê°€ ëœë‹¤. 

<span style="display:block;text-align:center">![relu](https://user-images.githubusercontent.com/59910975/115139252-c4491200-a06b-11eb-902f-311a7d597cc5.png)</span>

ReLU í•¨ìˆ˜ëŠ” ì´ë¯¸ì§€ ì—°ì‚°ì„ í•  ë•Œ ìœ ë¦¬í•œë°, ì´ë¯¸ì§€ëŠ” 0~255 ì‚¬ì´ì˜ RGB ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì´ë‹¤. í›ˆë ¨ ì¤‘ ìŒìˆ˜ê°€ ë‚˜ì™”ì„ ë•Œ ê·¸ ê°’ì— 0ì„ ê³±í•´ ë‹¤ìŒ Layerë¡œ ë„˜ì–´ê°€ì§€ ëª»í•˜ê²Œ ë§‰ëŠ”ë‹¤. 

ë˜í•œ ReLUëŠ” Loss Functionì˜ convexityë¥¼ ì–´ëŠ ì •ë„ ë³´ì¥í•œë‹¤. x=0ì„ ì œì™¸í•˜ê³ ëŠ” Linearí•œ ì„±ì§ˆì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

ëŒ€í‘œì ì¸ ë‹¨ì ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì¼ë¶€ Layerê°€ 0 ì´ì™¸ì˜ ê°’ì„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ” ê²½ìš°(ì´ë¥¼ ì£½ì—ˆë‹¤ê³  í‘œí˜„í•´ì„œ)ì¸ Dying ReLUê°€ ìˆë‹¤. íŠ¹íˆ í° Learning rateì„ ì‚¬ìš©í–ˆì„ ë•Œ ì‹ ê²½ë§ì˜ Layer ì ˆë°˜ì´ ì£½ì–´ìˆê¸°ë„ í•˜ë‹¤.

ğŸ‘‰ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜ë¡œ ë°”ë€Œì–´ Layerì—ì„œ ë‚´ë³´ë‚´ëŠ” ê°’ì´ ìŒìˆ˜ í•©ì´ ë˜ë©´ ReLU í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ë˜ë¯€ë¡œ ê²½ì‚¬í•˜ê°•ë²•ì´ ë”ì´ìƒ ì§„í–‰ë˜ì§€ ì•Šê²Œ ëœë‹¤.

ë‹¤ë¥¸ ë‹¨ì ìœ¼ë¡œëŠ”, ì–‘ìˆ˜ ê°€ì¤‘ì¹˜ê°€ ê³±í•´ì§„ë‹¤ê³  í•´ë„ ì´ì „ Layerì˜ ê°’ì— ReLU ê·¸ë˜ë””ì–¸íŠ¸ ê°’ 1ì´ ê³±í•´ì ¸ ê·¸ëŒ€ë¡œ ì „ë‹¬ë˜ê¸° ë•Œë¬¸ì— í° ê°’ì´ ì—°ì†í•´ì„œ ì—°ì‚° ë˜ëŠ” ìœ„í—˜ì´ ìˆë‹¤.

```python
# relu function
t = np.linspace(-10, 10, 100)
relu = np.maximum(t, 0)

# draw relu graph
plt.figure(figsize=(8,6))
plt.axhline(y=0, color="grey")
plt.axvline(color="grey")
plt.plot(t, relu, linewidth=2, label=r"$ReLU(x)=max(0, t)$", color="lawngreen")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("ReLU Activation Function")
plt.show()
```

      
           
               

## 4. Leaky ReLU

<center>$LeakyReLU(x) = \begin{cases}x & x\ge0 \\ scale*x & x< 0 \end{cases}$</center>

Leaky ReLUëŠ” Dying ReLUë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ê³ ì•ˆë˜ì—ˆë‹¤.
$LeakyReLU_a(z) = max(scale*z, z)$ë¡œ ì •ì˜ë˜ë©° $scale$ì€ í•¨ìˆ˜ê°€ ìƒˆëŠ” ì •ë„(Leaky)ë¥¼ ê²°ì •í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 0.1 í˜¹ì€ 0.01ë¡œ ì„¤ì •í•œë‹¤. 

ì´ ê¸°ìš¸ê¸°ê°’ì€ LeakyReLUê°€ ì£½ì§€ ì•Šê²Œ (0 ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê³±í•´ì§€ì§€ì•Šê²Œ) ë§Œë“¤ì–´ì¤€ë‹¤. 

<span style="display:block;text-align:center">![leaky_relu](https://user-images.githubusercontent.com/59910975/115139251-c4491200-a06b-11eb-9b93-f2aa6076fee1.png)</span>

[Bing Xu et al., 2015](https://arxiv.org/pdf/1505.00853.pdf) ì—ì„œ ì €ìëŠ” ì—¬ëŸ¬ ReLU ê³„ì—´ì˜ í•¨ìˆ˜ë¥¼ ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ì— ëŒ€í•´ ë¹„êµí•œ ê²°ê³¼ LeakyReLUê°€ ReLUë³´ë‹¤ í•­ìƒ ì„±ëŠ¥ì´ ë†’ë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ë‹¤. $scale$ ê°’ì„ $0.2$ë¡œ ë‘ì–´ $x<0$ ì¼ ë•Œ ë” í° ê°’ì„ í†µê³¼í•˜ê²Œ í•˜ëŠ” ê²ƒì´ $scale=0.01$ì¼ ë•Œë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” ì£¼ì–´ì§„ ë²”ìœ„ì—ì„œ $scale(a)$ê°’ì„ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ í›ˆë ¨ í›„, í…ŒìŠ¤íŠ¸í•  ë•Œ $a$ê°’ì˜ í‰ê· ì„ ë‚´ì–´ ì‚¬ìš©í•˜ëŠ” RReLU(randomized leaky ReLU)ì™€ 
$a$ê°’ì„ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë‘ê³  í•¨ê»˜ í•™ìŠµí•˜ì—¬ ì—…ë°ì´íŠ¸í•˜ëŠ” PReLU(parametric leaky ReLU)ë„ í•¨ê»˜ ì‹¤í—˜í–ˆë‹¤.

![relus](https://user-images.githubusercontent.com/59910975/115139253-c4e1a880-a06b-11eb-9a3b-f46a13df861f.png)
[https://arxiv.org/pdf/1505.00853.pdf](https://arxiv.org/pdf/1505.00853.pdf)

ê²°ê³¼ì ìœ¼ë¡œ PReLUì˜ ì„±ëŠ¥ì´ í•­ìƒ ì¢‹ì•˜ê³ , Leaky ReLUì™€ RReLU ì—­ì‹œ ReLUë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒˆë‹¤ê³  í•œë‹¤.

ë‹¤ë§Œ, ì‘ì€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” RReLUê°€ ì¼ì¢…ì˜ ê·œì œ ì—­í• ì„ í•´ì£¼ëŠ” ê²ƒìœ¼ë¡œ ë³´ì˜€ìœ¼ë©°, PReLUì˜ ê²½ìš° ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ê³¼ì í•©ë˜ëŠ”  ìœ„í—˜ì´ ìˆì—ˆë‹¤.

```python
# leaky relu function
t = np.linspace(-10, 10, 100)
scale = 0.1
leaky_relu = np.maximum(t, t*scale)

# draw leaky relu graph
plt.figure(figsize=(8,6))
plt.axhline(y=0, color="grey")
plt.axvline(color="grey")
plt.plot(t, leaky_relu , linewidth=2, label=r"$LeakyReLU(x)=max(scale*t, t), scale=0.1$", color = "lightseagreen")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("Leaky ReLU Activation Function")
plt.show()
```
    
      


## 5. ELU

<center>$$ELU_a(x) = \begin{cases}x & x \ge 0 \\ \alpha(e^x-1) & x < 0 \end{cases}$$</center>

[Djork-Ame Clevert et al, 2016](https://arxiv.org/pdf/1511.07289.pdf) ì— ì˜í•´ ì œì•ˆëœ ELUëŠ” ë‹¤ë¥¸ ëª¨ë“  ReLU ê³„ì—´ í™œì„± í•¨ìˆ˜ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆë‹¤. í›ˆë ¨ ì‹œê°„ì´ ì¤„ì—ˆì„ ë¿ ì•„ë‹ˆë¼ Test ë°ì´í„°ì—ì„œì˜ ì„±ëŠ¥ë„ ë” ë†’ì•˜ë‹¤. 

ìœ„ ì‹ì—ì„œ $a$ëŠ” $x$ê°€ í° ìŒìˆ˜ê°’ì¼ ë•Œ ELU í•¨ìˆ˜ê°€ ìˆ˜ë ´í•˜ëŠ” ê°’ì„ ì •ì˜í•œë‹¤. 

ì¼ë°˜ì ìœ¼ë¡œ $a=1$ë¡œ ì„¤ì •í•˜ì§€ë§Œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì²˜ëŸ¼ ë³€ê²½í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. $x < 0$ì¼ ë•Œì—ë„ ELUì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ì•„ë‹ˆë¯€ë¡œ Layerê°€ ì£½ì§€ ì•Šê²Œ í•œë‹¤. 

íŠ¹íˆ $a = 1$ì¼ ë•Œ ELUì˜ ë„í•¨ìˆ˜ëŠ” $x=0$ì—ì„œ ì—°ì†ì ì´ ë˜ì–´ ELUëŠ” ì „ì²´ êµ¬ê°„ì—ì„œ ë¯¸ë¶„ê°€ëŠ¥í•˜ê²Œ ëœë‹¤. 

ëª¨ë“  êµ¬ê°„ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë©´ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì†ë„ê°€ ë†’ì•„ì§„ë‹¤.

<span style="display:block;text-align:center">![elu](https://user-images.githubusercontent.com/59910975/115139250-c3b07b80-a06b-11eb-8bcc-9bdd09e88fd6.png)</span>

ELU í•¨ìˆ˜ì˜ ë‹¨ì ì€ ReLU ê³„ì—´ í•¨ìˆ˜ì— ë¹„í•´ ì—°ì‚° ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ê²ƒì´ë‹¤. 

$z < 0$ì¸ êµ¬ê°„ì—ì„œ ì§€ìˆ˜í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì¸ë°, í›ˆë ¨ ì‹œì—ëŠ” ìˆ˜ë ´ ì†ë„ê°€ ë¹¨ë¼ì„œ ì§€ìˆ˜í•¨ìˆ˜ì˜ ëŠë¦° ê³„ì‚°ì´ ìƒì‡„ë˜ì§€ë§Œ Test ë°ì´í„°ì—ì„œëŠ” ReLU í•¨ìˆ˜ë³´ë‹¤ ì—°ì‚°ì†ë„ê°€ ë” ëŠë¦¬ë‹¤.

```python
# elu function
t = np.linspace(-10, 10, 100)
alpha = 1
elu = np.where(t > 0, t, alpha * (np.exp(t) - 1))

# draw elu graph
plt.figure(figsize=(8,6))
plt.axhline(y=0, color="grey")
plt.axvline(color="grey")
plt.plot(t, elu, linewidth=2, label=r"$ELU_a(x), \alpha=1$", color="mediumslateblue")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("ELU Activation Function")
plt.show()
```
   
     


[Gunter Klambauer et al., 2017](https://arxiv.org/pdf/1706.02515.pdf) ì—ì„œëŠ” ì´ëŸ¬í•œ ELUì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” SELU(Scaled ELU) í™œì„± í•¨ìˆ˜ë¥¼ ì†Œê°œí•œë‹¤. 

<center>$$SELU(x) = \lambda \begin{cases} x & x > 0 \\ \alpha({e^x} - 1) & x \le 0 \end{cases}$$</center>

SELUëŠ” Fully Connected Layerì— ì‚¬ìš©ë ë•Œ ë„¤íŠ¸ì›Œí¬ê°€ Self-normalized ëœë‹¤ëŠ” íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤. 

Self-normalized ë˜ë©´ í•™ìŠµí•˜ëŠ” ë™ì•ˆ ê° Layerì˜ ì¶œë ¥ì´ 0ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ 1ì„ ìœ ì§€í•˜ê¸° ë•Œë¬¸ì— ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤, í­ì£¼ì˜ ìœ„í—˜ì„ ë°©ì§€í•œë‹¤.
ê·¸ ê²°ê³¼ SELUëŠ” ê¹Šì€ ì‹ ê²½ë§ì—ì„œ ë‹¤ë¥¸ í™œì„± í•¨ìˆ˜ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ê²Œëœë‹¤.

<span style="display:block;text-align:center">![selu](https://user-images.githubusercontent.com/59910975/115139254-c4e1a880-a06b-11eb-9fca-5f6e82094c67.png)</span>

ê·¸ëŸ¬ë‚˜ ë„¤íŠ¸ì›Œí¬ê°€ Self-normalize ë˜ê¸° ìœ„í•œ ì¡°ê±´ì´ ìˆë‹¤.

- ì…ë ¥ feature ì—­ì‹œ ì •ê·œí™”ë˜ì–´ì•¼ í•œë‹¤. (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
- ëª¨ë“  ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” ë¥´ì¿¤ ì´ˆê¸°í™”(í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ $\sigma^2 = {1 \over fan_{in}}$ì¸ ì •ê·œë¶„í¬) ë˜ì–´ì•¼ í•œë‹¤.
- ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Net)ì´ë‚˜ Skip-Connectionê³¼ ê°™ì´ ìˆœì°¨ì ìœ¼ë¡œ ì—°ì‚°í•˜ì§€ ì•ŠëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ëŠ” Self-normalizeë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤.

  
  
â˜ ë„¤íŠ¸ì›Œí¬ê°€ Self-normalize ë˜ì§€ ì•Šìœ¼ë©´ ë‹¤ë¥¸ í™œì„±í•¨ìˆ˜ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆë‹¤.

â˜ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ì—ì„œë„ SELUê°€ íš¨ê³¼ ìˆë‹¤ê³  í•œë‹¤.

```python
# selu function
t = np.linspace(-10, 10, 100)
# alpha, scale from https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu
alpha = 1.67326324
scale = 1.05070098
selu = scale*np.where(t > 0, t, alpha * (np.exp(t) - 1))

# draw selu graph
plt.figure(figsize=(8,6))
plt.axhline(y=0, color="grey")
plt.axvline(color="grey")
plt.plot(t, selu, linewidth=2, 
         label=r"$SELU(x), \alpha > 1, scale > 1$", color="mediumseagreen")
plt.xlim(-10, 10)
plt.xlabel("t")
plt.legend(fontsize=14)
plt.title("SELU Activation Function")
plt.show()
```
       



ì§€ê¸ˆê¹Œì§€ì˜ ReLU ê³„ì—´ í•¨ìˆ˜ë¥¼ í•œë²ˆì— ë¹„êµí•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<span style="display:block;text-align:center">![all_relus](https://user-images.githubusercontent.com/59910975/115139249-c27f4e80-a06b-11eb-9862-5f8d1ed4ae28.png)</span>

  
    
      

## ì–´ë–¤ í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?

ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ëŠ” í™œì„±í•¨ìˆ˜ëŠ”
SELU > ELU > Leaky RELU ê³„ì—´ í•¨ìˆ˜ë“¤ > ReLU > tanh > sigmoid ìˆœì´ë‹¤.

âœ… í•˜ì§€ë§Œ ë„¤íŠ¸ì›Œí¬ê°€ Self-normalizeë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë¼ë©´ SELUë³´ë‹¤ ELUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. \
âœ… ì¶”ë¡  ì†ë„ê°€ ì¤‘ìš”í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¤ê³„í•œë‹¤ë©´ LeakyReLUë¥¼ ê³ ë ¤í•œë‹¤.\
âœ… ë°ì´í„°ì…‹ì´ ì†Œê·œëª¨ì´ê³ , ì‹ ê²½ë§ì´ ê³¼ì í•© ë˜ì—ˆë‹¤ë©´ RReLUë¡œ í•™ìŠµì„ ê·œì œí•  ìˆ˜ ìˆë‹¤.\
âœ… ë°ì´í„°ì…‹ì´ ëŒ€ê·œëª¨ë¼ë©´ PReLUë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\
âœ… ReLU í™œì„± í•¨ìˆ˜ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ë¯€ë¡œ ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ëŠ” ReLUì— ìµœì í™”ë˜ì–´ ìˆë‹¤. í•™ìŠµ ì†ë„ê°€ ì¤‘ìš”í•˜ë‹¤ë©´ ReLUë¥¼ ê¶Œì¥í•œë‹¤.

  
    
      

Reference:

- [MathWorks](https://www.mathworks.com/help/deeplearning)
- Hands-On Machine Learning
- ë°ì´í„° ê³¼í•™ìì™€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ ì¸í„°ë·° ë¬¸ë‹µì§‘
- [Bing Xu et al., 2015](https://arxiv.org/pdf/1505.00853.pdf)
- [Gunter Klambauer et al., 2017](https://arxiv.org/pdf/1706.02515.pdf)
- [Djork-Ame Clevert et al, 2016](https://arxiv.org/pdf/1511.07289.pdf)
- [https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)